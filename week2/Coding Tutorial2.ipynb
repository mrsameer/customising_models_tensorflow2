{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flow from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daisy  dandelion  rose\tsunflower  tulip\r\n"
     ]
    }
   ],
   "source": [
    "! ls data/flowers-recognition-split/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the directory structure\n",
    "\n",
    "train_path = 'data/flowers-recognition-split/train'\n",
    "val_path = 'data/flowers-recognition-split/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator object\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagenerator = ImageDataGenerator(rescale=(1/255.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3027 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create a training data generator\n",
    "\n",
    "train_generator = datagenerator.flow_from_directory(\n",
    "    train_path, \n",
    "    batch_size = 64, \n",
    "    classes = classes, \n",
    "    target_size = [16, 16]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1296 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create a validation data generator\n",
    "\n",
    "validation_generator = datagenerator.flow_from_directory(\n",
    "    val_path, \n",
    "    batch_size = 64, \n",
    "    classes = classes, \n",
    "    target_size = [16, 16]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get and display an image and label from the training generator\n",
    "\n",
    "x = next(train_generator)\n",
    "matplotlib.pyplot.imshow(x[0][4])\n",
    "print(x[1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3027 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Reset the training generator\n",
    "\n",
    "train_generator = datagenerator.flow_from_directory(\n",
    "    train_path, \n",
    "    batch_size = 64, \n",
    "    classes = classes, \n",
    "    target_size = [16, 16]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a model to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build a CNN model\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Input((16,16,3)))\n",
    "model.add(Conv2D(8, (8, 8), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((4,4)))\n",
    "model.add(Conv2D(8, (8, 8), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(4, (4, 4), padding='same', activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer object\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 16, 16, 8)         1544      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 8)           4104      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 2, 4)           516       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 6,617\n",
      "Trainable params: 6,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 20\n"
     ]
    }
   ],
   "source": [
    "# Calculate the training generator and test generator steps per epoch\n",
    "\n",
    "train_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "val_steps = validation_generator.n // validation_generator.batch_size\n",
    "print(train_steps_per_epoch, val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd2d339b5c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch = train_steps_per_epoch, \n",
    "    epochs = 5, \n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4081502318382264, 0.42109376]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "model.evaluate_generator(\n",
    "    validation_generator, \n",
    "    steps = val_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels with the model\n",
    "\n",
    "predictions = model.predict_generator(\n",
    "    validation_generator, \n",
    "    steps = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19370535, 0.24312106, 0.1954575 , 0.1617609 , 0.2059552 ],\n",
       "       [0.19905609, 0.08694265, 0.22791189, 0.15128992, 0.3347995 ],\n",
       "       [0.16040964, 0.03248033, 0.22539692, 0.14693452, 0.4347785 ],\n",
       "       [0.11529136, 0.00591432, 0.19550818, 0.09856845, 0.5847177 ],\n",
       "       [0.20709261, 0.241492  , 0.16514252, 0.18816113, 0.1981117 ],\n",
       "       [0.16011898, 0.01695566, 0.2172231 , 0.10724498, 0.49845725],\n",
       "       [0.20043975, 0.18027478, 0.210485  , 0.16248152, 0.24631904],\n",
       "       [0.14934082, 0.05645678, 0.19911239, 0.23795101, 0.35713893],\n",
       "       [0.13887323, 0.00802662, 0.2025639 , 0.08812384, 0.5624124 ],\n",
       "       [0.22792457, 0.23438476, 0.11030822, 0.25451222, 0.17287022],\n",
       "       [0.19930214, 0.19406292, 0.2073106 , 0.1627077 , 0.23661664],\n",
       "       [0.19611546, 0.24247108, 0.19070093, 0.16544238, 0.20527019],\n",
       "       [0.20005257, 0.19639282, 0.20476124, 0.16417389, 0.2346195 ],\n",
       "       [0.10568874, 0.02568201, 0.1607395 , 0.32065865, 0.38723108],\n",
       "       [0.16184907, 0.01802017, 0.21825607, 0.10888977, 0.49298492],\n",
       "       [0.20100467, 0.17207107, 0.21232995, 0.16222312, 0.25237116],\n",
       "       [0.21639705, 0.1472698 , 0.13830578, 0.2620107 , 0.23601675],\n",
       "       [0.15776661, 0.10889976, 0.1228856 , 0.3767062 , 0.23374186],\n",
       "       [0.07991331, 0.0556983 , 0.04922039, 0.67403156, 0.14113645],\n",
       "       [0.19796568, 0.20774032, 0.20408385, 0.16269924, 0.22751097],\n",
       "       [0.11896797, 0.03780254, 0.17894123, 0.29440698, 0.3698812 ],\n",
       "       [0.08100798, 0.03844541, 0.074968  , 0.5961481 , 0.2094305 ],\n",
       "       [0.04232721, 0.01250405, 0.04723056, 0.71295875, 0.18497941],\n",
       "       [0.08230201, 0.01689138, 0.15037568, 0.35272843, 0.3977025 ],\n",
       "       [0.20514362, 0.18806021, 0.19695763, 0.17186621, 0.23797229],\n",
       "       [0.08851512, 0.01454292, 0.19973938, 0.22821754, 0.46898505],\n",
       "       [0.20221826, 0.14176115, 0.21878256, 0.16031952, 0.27691853],\n",
       "       [0.0396071 , 0.02389119, 0.02169974, 0.8277321 , 0.08706989],\n",
       "       [0.18097205, 0.03627041, 0.22745997, 0.12831098, 0.42698658],\n",
       "       [0.11075729, 0.03277271, 0.13776188, 0.3785708 , 0.3401373 ],\n",
       "       [0.198449  , 0.08323253, 0.22832265, 0.15028378, 0.33971205],\n",
       "       [0.14325936, 0.07816385, 0.13139798, 0.38108516, 0.2660936 ],\n",
       "       [0.19581573, 0.22668771, 0.19950806, 0.16234842, 0.21564004],\n",
       "       [0.2017047 , 0.15912609, 0.21516332, 0.16160606, 0.26239982],\n",
       "       [0.1813804 , 0.07716981, 0.22771962, 0.16823834, 0.3454918 ],\n",
       "       [0.20208958, 0.1482244 , 0.21746285, 0.16086514, 0.27135804],\n",
       "       [0.20585704, 0.23987475, 0.17087464, 0.18218663, 0.20120692],\n",
       "       [0.19879848, 0.19945587, 0.2060468 , 0.16273062, 0.23296823],\n",
       "       [0.19049212, 0.09005499, 0.22551912, 0.16486037, 0.32907343],\n",
       "       [0.07476376, 0.00149626, 0.1629616 , 0.08039102, 0.6803874 ],\n",
       "       [0.19151947, 0.05668575, 0.22973974, 0.14059532, 0.38145974],\n",
       "       [0.1998499 , 0.1877605 , 0.20877221, 0.1626352 , 0.2409822 ],\n",
       "       [0.20220634, 0.14258073, 0.21861719, 0.16039339, 0.27620244],\n",
       "       [0.14680137, 0.0269997 , 0.22238468, 0.15399936, 0.44981486],\n",
       "       [0.1958468 , 0.24254645, 0.19122909, 0.16502859, 0.20534903],\n",
       "       [0.19905238, 0.24159816, 0.18495914, 0.17002346, 0.20436685],\n",
       "       [0.20299368, 0.158585  , 0.21268955, 0.16341805, 0.26231366],\n",
       "       [0.21358898, 0.19667162, 0.17320713, 0.19149002, 0.22504234],\n",
       "       [0.09872287, 0.01310712, 0.20501353, 0.18809544, 0.49506095],\n",
       "       [0.2022555 , 0.13807397, 0.21951915, 0.15996985, 0.28018156],\n",
       "       [0.09956533, 0.07995148, 0.05615579, 0.6192559 , 0.14507145],\n",
       "       [0.17539304, 0.02933988, 0.22526981, 0.12236632, 0.44763088],\n",
       "       [0.16658278, 0.04854435, 0.2277784 , 0.16356574, 0.3935287 ],\n",
       "       [0.2019033 , 0.18179229, 0.20653437, 0.16537496, 0.2443951 ],\n",
       "       [0.20031445, 0.09641208, 0.22670253, 0.15357065, 0.3230003 ],\n",
       "       [0.04402387, 0.01691656, 0.03996661, 0.7469772 , 0.15211572],\n",
       "       [0.09421283, 0.01339146, 0.20359723, 0.20132953, 0.48746887],\n",
       "       [0.20224455, 0.15836696, 0.21397844, 0.16274942, 0.26266062],\n",
       "       [0.19634217, 0.07284229, 0.22924055, 0.14706998, 0.35450503],\n",
       "       [0.2042562 , 0.20287327, 0.19319691, 0.1716918 , 0.22798184],\n",
       "       [0.20105179, 0.17132182, 0.21249667, 0.1621946 , 0.25293514],\n",
       "       [0.19370535, 0.24312106, 0.1954575 , 0.1617609 , 0.2059552 ],\n",
       "       [0.19373107, 0.24293   , 0.195505  , 0.1617691 , 0.20606484],\n",
       "       [0.20029208, 0.18223271, 0.21003956, 0.16252905, 0.24490662]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_4\"></a>\n",
    "## The Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "dataset  = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (2,), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([[1,2], [3, 4], [5,6 ]])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (5,), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.random.uniform([128, 5]))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((4,), ()), types: (tf.int32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform([256, 4], minval = 1, maxval = 10, dtype = tf.int32), \n",
    "    tf.random.normal([256]))\n",
    ") # inputs and outputs \n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(1,), dtype=tf.uint8, name=None))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_datagen = ImageDataGenerator(\n",
    "    width_shift_range = 0.2, \n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    img_datagen.flow, \n",
    "    args = [x_train, y_train], \n",
    "    output_types = (tf.float32, tf.int32), \n",
    "    output_shapes = ([32, 32, 32, 3], [32, 1])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((100,10,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the tensor x\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(10, 2, 2), dtype=tf.float64, name=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "dataset1.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = [np.zeros((10,2,2)), np.zeros((5,2,2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e4370cf142de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Try creating a dataset from the tensor x2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    433\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \"\"\"\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   2352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2354\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2355\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 111\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    112\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    284\u001b[0m                                          as_ref=False):\n\u001b[1;32m    285\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \"\"\"\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "# Try creating a dataset from the tensor x2\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(x2)  # this does not work as elements are of different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = [np.zeros((10,1)), np.zeros((10,1)), np.zeros((10,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataset from the new x2 and inspect the Dataset object\n",
    "\n",
    "dataset3 = tf.data.Dataset.from_tensor_slices(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(10, 1), dtype=tf.float64, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "print(dataset3.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a zipped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two datasets into one larger dataset\n",
    "\n",
    "dataset_zipped = tf.data.Dataset.zip((dataset1, dataset3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(10, 2, 2), dtype=tf.float64, name=None), TensorSpec(shape=(10, 1), dtype=tf.float64, name=None))\n"
     ]
    }
   ],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "print(dataset_zipped.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the number of batches in a dataset\n",
    "\n",
    "def get_batches(dataset):\n",
    "    iter_dataset = iter(dataset)\n",
    "    i = 0\n",
    "    try:\n",
    "        while next(iter_dataset):\n",
    "            i = i+1\n",
    "    except:\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of batches in the zipped Dataset\n",
    "\n",
    "get_batches(dataset_zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset from numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "\n",
    "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(type(train_features), type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset from the MNIST data\n",
    "\n",
    "mnist_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n"
     ]
    }
   ],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "print(mnist_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the length of an element using the take method\n",
    "\n",
    "element = next(iter(mnist_dataset.take(1)))\n",
    "len(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# Examine the shapes of the data\n",
    "\n",
    "print(element[0].shape)\n",
    "print(element[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset from text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/shakespeare/tempest.1.1.txt', 'data/shakespeare/tempest.1.2.txt', 'data/shakespeare/tempest.2.1.txt', 'data/shakespeare/tempest.2.2.txt', 'data/shakespeare/tempest.3.1.txt', 'data/shakespeare/tempest.3.2.txt', 'data/shakespeare/tempest.3.3.txt', 'data/shakespeare/tempest.4.1.txt', 'data/shakespeare/tempest.5.1.txt']\n"
     ]
    }
   ],
   "source": [
    "# Print the list of text files\n",
    "\n",
    "text_files = sorted([f.path for f in os.scandir('data/shakespeare')])\n",
    "\n",
    "print(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENE I. On a ship at sea: a tempestuous noise\n",
      "\n",
      "of thunder and lightning heard.\n",
      "\n",
      "Enter a Master and a Boatswain\n",
      "\n",
      "\n",
      "\n",
      "Master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the first file using python and print the first 5 lines.\n",
    "\n",
    "with open(text_files[0], 'r') as fil:\n",
    "    contents = [fil.readline() for i in range(5)]\n",
    "    for line in contents:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lines from the files into a dataset using TextLineDataset\n",
    "\n",
    "shakespeare_dataset = tf.data.TextLineDataset(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'SCENE I. On a ship at sea: a tempestuous noise', shape=(), dtype=string)\n",
      "tf.Tensor(b'of thunder and lightning heard.', shape=(), dtype=string)\n",
      "tf.Tensor(b'Enter a Master and a Boatswain', shape=(), dtype=string)\n",
      "tf.Tensor(b'', shape=(), dtype=string)\n",
      "tf.Tensor(b'Master', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Use the take method to get and print the first 5 lines of the dataset\n",
    "\n",
    "first_5_lines_dataset = iter(shakespeare_dataset.take(5))\n",
    "lines = [line for line in first_5_lines_dataset]\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of lines in the first file\n",
    "\n",
    "lines = []\n",
    "with open(text_files[0], 'r') as fil:\n",
    "    line = fil.readline()\n",
    "    while line:\n",
    "        lines.append(line)\n",
    "        line = fil.readline()\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3134\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of lines in the shakespeare dataset we created\n",
    "\n",
    "shakespeare_dataset_iterator = iter(shakespeare_dataset)\n",
    "lines = [line for line in shakespeare_dataset_iterator]\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interleave lines from the text data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'data/shakespeare/tempest.1.1.txt', shape=(), dtype=string)\n",
      "tf.Tensor(b'data/shakespeare/tempest.1.2.txt', shape=(), dtype=string)\n",
      "tf.Tensor(b'data/shakespeare/tempest.2.1.txt', shape=(), dtype=string)\n",
      "tf.Tensor(b'data/shakespeare/tempest.2.2.txt', shape=(), dtype=string)\n",
      "tf.Tensor(b'data/shakespeare/tempest.3.1.txt', shape=(), dtype=string)\n",
      "tf.Tensor(b'data/shakespeare/tempest.3.2.txt', shape=(), dtype=string)\n",
      "tf.Tensor(b'data/shakespeare/tempest.3.3.txt', shape=(), dtype=string)\n",
      "tf.Tensor(b'data/shakespeare/tempest.4.1.txt', shape=(), dtype=string)\n",
      "tf.Tensor(b'data/shakespeare/tempest.5.1.txt', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset of the text file strings\n",
    "\n",
    "text_files_dataset = tf.data.Dataset.from_tensor_slices(text_files)\n",
    "files = [file for file in text_files_dataset]\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(), dtype=tf.string, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Interleave the lines from the text files\n",
    "\n",
    "interleaved_shakespeare_dataset = text_files_dataset.interleave(tf.data.TextLineDataset, cycle_length = 9)\n",
    "print(interleaved_shakespeare_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'SCENE I. On a ship at sea: a tempestuous noise', shape=(), dtype=string)\n",
      "tf.Tensor(b\"SCENE II. The island. Before PROSPERO'S cell.\", shape=(), dtype=string)\n",
      "tf.Tensor(b'SCENE I. Another part of the island.', shape=(), dtype=string)\n",
      "tf.Tensor(b'SCENE II. Another part of the island.', shape=(), dtype=string)\n",
      "tf.Tensor(b\"SCENE I. Before PROSPERO'S Cell.\", shape=(), dtype=string)\n",
      "tf.Tensor(b'SCENE II. Another part of the island.', shape=(), dtype=string)\n",
      "tf.Tensor(b'SCENE III. Another part of the island.', shape=(), dtype=string)\n",
      "tf.Tensor(b\"SCENE I. Before PROSPERO'S cell.\", shape=(), dtype=string)\n",
      "tf.Tensor(b\"SCENE I. Before PROSPERO'S cell.\", shape=(), dtype=string)\n",
      "tf.Tensor(b'of thunder and lightning heard.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 elements of the interleaved dataset\n",
    "\n",
    "lines = [line for line in iter(interleaved_shakespeare_dataset.take(10))]\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_5\"></a>\n",
    "## Training with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the UCI Bank Marketing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a pandas DataFrame\n",
    "\n",
    "bank_dataframe = pd.read_csv('data/bank/bank-full.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "bank_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45211, 17)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the DataFrame\n",
    "\n",
    "print(bank_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features from the DataFrame\n",
    "\n",
    "features = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
    "            'loan', 'contact', 'campaign', 'pdays', 'poutcome']\n",
    "labels = ['y']\n",
    "\n",
    "bank_dataframe = bank_dataframe.filter(features + labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  campaign  pdays poutcome   y  \n",
       "0  unknown         1     -1  unknown  no  \n",
       "1  unknown         1     -1  unknown  no  \n",
       "2  unknown         1     -1  unknown  no  \n",
       "3  unknown         1     -1  unknown  no  \n",
       "4  unknown         1     -1  unknown  no  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "bank_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical features in the DataFrame to one-hot encodings\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "categorical_features = ['default', 'housing', 'job', 'loan', 'education', 'contact', 'poutcome']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    bank_dataframe[feature] = tuple(encoder.fit_transform(bank_dataframe[feature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>(0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>married</td>\n",
       "      <td>(0, 0, 1, 0)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>2143</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>(0, 0, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(0, 0, 0, 1)</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0)</td>\n",
       "      <td>single</td>\n",
       "      <td>(0, 1, 0, 0)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>29</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>(0, 0, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(0, 0, 0, 1)</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>(0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>married</td>\n",
       "      <td>(0, 1, 0, 0)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>2</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>(0, 0, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(0, 0, 0, 1)</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</td>\n",
       "      <td>married</td>\n",
       "      <td>(0, 0, 0, 1)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>1506</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>(0, 0, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(0, 0, 0, 1)</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)</td>\n",
       "      <td>single</td>\n",
       "      <td>(0, 0, 0, 1)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>1</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>(0, 0, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(0, 0, 0, 1)</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                                   job  marital     education default  \\\n",
       "0   58  (0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0)  married  (0, 0, 1, 0)    (0,)   \n",
       "1   44  (0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0)   single  (0, 1, 0, 0)    (0,)   \n",
       "2   33  (0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)  married  (0, 1, 0, 0)    (0,)   \n",
       "3   47  (0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)  married  (0, 0, 0, 1)    (0,)   \n",
       "4   33  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)   single  (0, 0, 0, 1)    (0,)   \n",
       "\n",
       "   balance housing  loan    contact  campaign  pdays      poutcome   y  \n",
       "0     2143    (1,)  (0,)  (0, 0, 1)         1     -1  (0, 0, 0, 1)  no  \n",
       "1       29    (1,)  (0,)  (0, 0, 1)         1     -1  (0, 0, 0, 1)  no  \n",
       "2        2    (1,)  (1,)  (0, 0, 1)         1     -1  (0, 0, 0, 1)  no  \n",
       "3     1506    (1,)  (0,)  (0, 0, 1)         1     -1  (0, 0, 0, 1)  no  \n",
       "4        1    (0,)  (0,)  (0, 0, 1)         1     -1  (0, 0, 0, 1)  no  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the head of the DataFrame\n",
    "\n",
    "bank_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "\n",
    "bank_dataframe = bank_dataframe.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a Dataset\n",
    "\n",
    "bank_dataset = tf.data.Dataset.from_tensor_slices(dict(bank_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'job': TensorSpec(shape=(12,), dtype=tf.int32, name=None),\n",
       " 'marital': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'education': TensorSpec(shape=(4,), dtype=tf.int32, name=None),\n",
       " 'default': TensorSpec(shape=(1,), dtype=tf.int32, name=None),\n",
       " 'balance': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'housing': TensorSpec(shape=(1,), dtype=tf.int32, name=None),\n",
       " 'loan': TensorSpec(shape=(1,), dtype=tf.int32, name=None),\n",
       " 'contact': TensorSpec(shape=(3,), dtype=tf.int32, name=None),\n",
       " 'campaign': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'pdays': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'poutcome': TensorSpec(shape=(4,), dtype=tf.int32, name=None),\n",
       " 'y': TensorSpec(shape=(), dtype=tf.string, name=None)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "bank_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a person with marital status: b'single'\n"
     ]
    }
   ],
   "source": [
    "# First check that there are records in the dataset for non-married individuals\n",
    "\n",
    "def check_divorced():\n",
    "    bank_dataset_iterable = iter(bank_dataset)\n",
    "    for x in bank_dataset_iterable:\n",
    "        if x['marital'] != 'divorced':\n",
    "            print('Found a person with marital status: {}'.format(x['marital']))\n",
    "            return\n",
    "    print('No non-divorced people were found!')\n",
    "\n",
    "check_divorced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the Dataset to retain only entries with a 'divorced' marital status\n",
    "\n",
    "bank_dataset = bank_dataset.filter(lambda x : tf.equal(x['marital'], tf.constant([b'divorced']))[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No non-divorced people were found!\n"
     ]
    }
   ],
   "source": [
    "# Check the records in the dataset again\n",
    "\n",
    "check_divorced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map a function over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the label ('y') to an integer instead of 'yes' or 'no'\n",
    "\n",
    "def map_label(x):\n",
    "    x['y'] = 0 if (x['y'] == tf.constant([b'no'], dtype = tf.string)) else 1\n",
    "    return x\n",
    "\n",
    "bank_dataset = bank_dataset.map(map_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'job': TensorSpec(shape=(12,), dtype=tf.int32, name=None),\n",
       " 'marital': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'education': TensorSpec(shape=(4,), dtype=tf.int32, name=None),\n",
       " 'default': TensorSpec(shape=(1,), dtype=tf.int32, name=None),\n",
       " 'balance': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'housing': TensorSpec(shape=(1,), dtype=tf.int32, name=None),\n",
       " 'loan': TensorSpec(shape=(1,), dtype=tf.int32, name=None),\n",
       " 'contact': TensorSpec(shape=(3,), dtype=tf.int32, name=None),\n",
       " 'campaign': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'pdays': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'poutcome': TensorSpec(shape=(4,), dtype=tf.int32, name=None),\n",
       " 'y': TensorSpec(shape=(), dtype=tf.int32, name=None)}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "bank_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'marital' column\n",
    "bank_dataset = bank_dataset.map(lambda x : {key:val for key,val in x.items() if key != 'marital'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'job': TensorSpec(shape=(12,), dtype=tf.int32, name=None),\n",
       " 'education': TensorSpec(shape=(4,), dtype=tf.int32, name=None),\n",
       " 'default': TensorSpec(shape=(1,), dtype=tf.int32, name=None),\n",
       " 'balance': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'housing': TensorSpec(shape=(1,), dtype=tf.int32, name=None),\n",
       " 'loan': TensorSpec(shape=(1,), dtype=tf.int32, name=None),\n",
       " 'contact': TensorSpec(shape=(3,), dtype=tf.int32, name=None),\n",
       " 'campaign': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'pdays': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'poutcome': TensorSpec(shape=(4,), dtype=tf.int32, name=None),\n",
       " 'y': TensorSpec(shape=(), dtype=tf.int32, name=None)}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "bank_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create input and output data tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input and output tuple for the dataset\n",
    "\n",
    "def map_feature_label(x):\n",
    "    features = [[x['age']], [x['balance']], [x['campaign']], x['contact'], x['default'],\n",
    "                x['education'], x['housing'], x['job'], x['loan'], [x['pdays']], x['poutcome']]\n",
    "    return (tf.concat(features, axis=0), x['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Map this function over the dataset\n",
    "\n",
    "bank_dataset = bank_dataset.map(map_feature_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(30,), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the Dataset object\n",
    "\n",
    "bank_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into a training and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5207\n"
     ]
    }
   ],
   "source": [
    "# Determine the length of the Dataset\n",
    "\n",
    "dataset_length = 0\n",
    "for _ in bank_dataset:\n",
    "    dataset_length += 1\n",
    "print(dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and validation sets from the dataset\n",
    "\n",
    "training_elements = int(dataset_length + 0.7)\n",
    "train_dataset = bank_dataset.take(training_elements)\n",
    "validation_dataset = bank_dataset.skip(training_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a classification model\n",
    "\n",
    "Now let's build a model to classify the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a classifier model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, BatchNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(30,)))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 400)               12400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 176,521\n",
      "Trainable params: 174,861\n",
      "Non-trainable params: 1,660\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batched training and validation datasets\n",
    "\n",
    "train_dataset = train_dataset.batch(20, drop_remainder = True)\n",
    "validation_dataset = validation_dataset.batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training data\n",
    "\n",
    "train_dataset = train_dataset.shuffle(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "260/260 [==============================] - 24s 93ms/step - loss: 0.4380 - accuracy: 0.8538\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m:  End of sequence\n\t [[node IteratorGetNext (defined at /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_148039]\n\nFunction call stack:\ndistributed_function\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-632c33cc20cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                       total_epochs=1)\n\u001b[0m\u001b[1;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m    372\u001b[0m                                  prefix='val_')\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    144\u001b[0m               \u001b[0;34m'{} batches). You may need to use the repeat() function '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m               'when building your dataset.'.format(\n\u001b[0;32m--> 146\u001b[0;31m                   total_epochs * steps_per_epoch))\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;31m# In either case, break out the loop for training batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# Also note the training_context that data inputs are exhausted, so all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(train_dataset, validation_data = validation_dataset, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "plt.plot(history.epoch, history.history['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
